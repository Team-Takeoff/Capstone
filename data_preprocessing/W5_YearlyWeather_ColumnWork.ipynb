{"cells":[{"cell_type":"markdown","id":"e45ceec0-150f-4c54-9126-eb6dbd3811d8","metadata":{"id":"e45ceec0-150f-4c54-9126-eb6dbd3811d8"},"source":["## Work with Weather Columns\n","<br>\n","This code works with the weather data, expanding columns, narrowing down to the columns for our models and fills missing values."]},{"cell_type":"code","execution_count":null,"id":"4b6da3c6-df20-4fdb-aaae-839bb0a0d8d8","metadata":{"id":"4b6da3c6-df20-4fdb-aaae-839bb0a0d8d8","outputId":"f0f4ad96-fc4e-440a-fbcf-51b34f923b66"},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["import pandas as pd\n","basepath = 'your_path_here'"]},{"cell_type":"markdown","id":"c52975b7","metadata":{"id":"c52975b7"},"source":["# Creating the weather files with the columns determined to be most useful\n","This code creates a yearly weather files with the columns we determined after adding the data to the OnTime Keys with Weather Delay/No-Delay flag to see which ones look like they offer more data.<br>\n","<br>\n","The next step/notebook will narrow these down to just the DFW and DFW-Destinations and then work with the duplicate weather records to narrow down to unique records per Date/Time"]},{"cell_type":"code","execution_count":null,"id":"c3846f1e","metadata":{"id":"c3846f1e"},"outputs":[],"source":["# Get list of airports to filter down to\n","destdatetime = pd.read_parquet(basepath + \"/OnTime/destdatetime_dttm_dfw_uniq.parquet\")\n","airport_codes = destdatetime['Dest'].unique().tolist()\n","airport_codes.append('DFW')\n","print(\"# of airport codes:\", len(airport_codes))"]},{"cell_type":"code","execution_count":null,"id":"157b64f6","metadata":{"id":"157b64f6"},"outputs":[],"source":["# This is basically the same steps used to create the datetime column for the OnTime keys\n","def createYMD(date):\n","    year = ''\n","    month = ''\n","    day = ''\n","    hour = ''\n","    minute = ''\n","    for i in range(4):\n","        year = year + date[i]\n","    for i in range(5,7):\n","        month = month + date[i]\n","    for i in range(8,10):\n","        day = day + date[i]\n","    for i in range(11,13):\n","        hour = hour + date[i]\n","    for i in range(14,16):\n","        minute = minute + date[i]\n","    OriginDtTm1 = year + month + day + hour + minute\n","    WeatherDtTm = pd.to_datetime(OriginDtTm1, format= '%Y%m%d%H%M', errors='coerce')\n","    return WeatherDtTm #year, month, day, hour, minute\n","\n","# This function is used to open each of the yearly weather files and apply the function above creating the datetime\n","def createYrWeather(df, year):\n","    df = pd.read_pickle(basepath + \"/Weather/Hourly/Weather_year_pkl_FirstTimeThrough/Weather_\"+ year+\".pkl\")\n","    df['DateTime_charlist'] = df['DATE'].apply(lambda x: [str(char) for char in x]).copy()\n","    df['WeatherDtTm'] = df.apply(lambda x: createYMD(x['DateTime_charlist']), axis = 1, result_type=\"expand\").copy()\n","    df.drop(['DateTime_charlist'], axis = 1, inplace=True)\n","    return df"]},{"cell_type":"markdown","id":"053bfabc","metadata":{"id":"053bfabc"},"source":["There are many columns in the weather data with several columns comma separated within them.<br>\n","These columns need to be split and expanded into their own columns. We don't need everything in each original column but we have to split it to get the column out we do want."]},{"cell_type":"code","execution_count":null,"id":"ece678fe","metadata":{"id":"ece678fe"},"outputs":[],"source":["def updateWeatherCols(wind, cig, vis, tmp, dew, slp, aa1, ga1, gd1, gf1, ma1):\n","    wind = str(wind)\n","    cig = str(cig)\n","    vis = str(vis)\n","    tmp = str(tmp)\n","    dew = str(dew)\n","    slp = str(slp)\n","    aa1 = str(aa1)\n","    ga1 = str(ga1)\n","    gd1 = str(gd1)\n","    gf1 = str(gf1)\n","    ma1 = str(ma1)\n","    if wind.count(\",\") == 4:\n","        w_dir_angle, w_dir_qlty, w_type, w_speed_rate, w_speed_qlty = wind.split(\",\")\n","    else:\n","        w_dir_angle, w_dir_qlty, w_type, w_speed_rate, w_speed_qlty = 999, None, 9, 9999, None\n","\n","    if cig.count(\",\") == 3:\n","        sky_c_hgt, sky_c_qlty, sky_c_det, sky_c_cavok = cig.split(\",\")\n","    else:\n","        sky_c_hgt, sky_c_qlty, sky_c_det, sky_c_cavok = 99999, None, 9, 9\n","\n","    if vis.count(\",\") == 3:\n","        vis_dist, vis_dist_qlty, vis_var, vis_var_qlty = vis.split(\",\")\n","    else:\n","        vis_dist, vis_dist_qlty, vis_var, vis_var_qlty = 999999, None, 9, None\n","\n","    if tmp.count(\",\") == 1:\n","        tmp_air, tmp_air_qlty = tmp.split(\",\")\n","    else:\n","        tmp_air, tmp_air_qlty = +9999, None\n","\n","    if dew.count(\",\") == 1:\n","        tmp_dew, tmp_dew_qlty = dew.split(\",\")\n","    else:\n","        tmp_dew, tmp_dew_qlty = +9999, None\n","\n","    if slp.count(\",\") == 1:\n","        sea_lvl_p, sea_lvl_p_qlty = slp.split(\",\")\n","    else:\n","        sea_lvl_p, sea_lvl_p_qlty = 99999, 99\n","\n","    if aa1.count(\",\") == 3:\n","        liq_precip_qty, liq_precip_dim, liq_precip_cond, liq_precip_qlty = aa1.split(\",\")\n","    else:\n","        liq_precip_qty, liq_precip_dim, liq_precip_cond, liq_precip_qlty = 99, 9999, 9, None\n","\n","    if ga1.count(\",\") == 5:\n","        sky_cov, sky_cov_qlty, sky_cov_base_hgt, sky_cov_base_qlty, sky_cov_cld, sky_cov_cld_qlty = ga1.split(\",\")\n","    else:\n","        sky_cov, sky_cov_qlty, sky_cov_base_hgt, sky_cov_base_qlty, sky_cov_cld, sky_cov_cld_qlty = 99, None, +99999, None, 99, None\n","\n","    if gd1.count(\",\") == 5:\n","        sky_sum_cov, sky_sum_cov2, sky_sum_cov_qlty, sky_sum_hgt, sky_sum_hgt_qlty, sky_sum_char = gd1.split(\",\")\n","    else:\n","        sky_sum_cov, sky_sum_cov2, sky_sum_cov_qlty, sky_sum_hgt, sky_sum_hgt_qlty, sky_sum_char = 9, 99, None, +99999, 99999, None\n","\n","    if gf1.count(\",\") == 12:\n","        sky_obs_tot_cov, sky_obs_tot_opaq, sky_obs_qlty_tot_cov, sky_obs_tot_low_cld, sky_obs_qlty_tot_low, \\\n","        sky_low_cld_genus, sky_qlty_low_cld_genus, sky_low_cld_base_hgt, sky_low_cld_base_hgt_qlty, \\\n","        sky_mid_cld_genus, sky_qlty_mid_cld_genus, sky_hi_cld_genus, sky_qlty_hi_cld_genus = gf1.split(\",\")\n","    else:\n","        sky_obs_tot_cov, sky_obs_tot_opaq, sky_obs_qlty_tot_cov, sky_obs_tot_low_cld, sky_obs_qlty_tot_low, \\\n","        sky_low_cld_genus, sky_qlty_low_cld_genus, sky_low_cld_base_hgt, sky_low_cld_base_hgt_qlty, \\\n","        sky_mid_cld_genus, sky_qlty_mid_cld_genus, sky_hi_cld_genus, sky_qlty_hi_cld_genus\\\n","            = 99, None, 99, None, None,\\\n","                None, None, 99999, 99,\\\n","                None, None, None, None\n","\n","    if ma1.count(\",\") == 3:\n","        at_pres_altimeter_rate, at_pres_altimeter_qlty, at_pres_stn_rate, at_pres_stn_qlty = ma1.split(\",\")\n","    else:\n","        at_pres_altimeter_rate, at_pres_altimeter_qlty, at_pres_stn_rate, at_pres_stn_qlty = 99999, None, 99999, None\n","\n","    return w_dir_angle, w_type, w_speed_rate, sky_c_hgt, sky_c_det, sky_c_cavok,\\\n","            vis_dist, vis_var, tmp_air, tmp_dew, sea_lvl_p, \\\n","            liq_precip_qty, liq_precip_dim, liq_precip_cond, sky_cov, sky_cov_base_hgt, sky_cov_cld, \\\n","            sky_sum_cov, sky_sum_hgt, sky_obs_tot_cov,\\\n","            sky_low_cld_base_hgt, at_pres_altimeter_rate, at_pres_stn_rate"]},{"cell_type":"code","execution_count":null,"id":"a822a8a1","metadata":{"id":"a822a8a1"},"outputs":[],"source":["def expandColsAndRunFreqs(df):\n","    df[['w_dir_angle', 'w_type', 'w_speed_rate', 'sky_c_hgt', 'sky_c_det', 'sky_c_cavok',\\\n","            'vis_dist', 'vis_var', 'tmp_air', 'tmp_dew', 'sea_lvl_p', \\\n","            'liq_precip_qty', 'liq_precip_dim', 'liq_precip_cond', 'sky_cov', 'sky_cov_base_hgt', 'sky_cov_cld', \\\n","            'sky_sum_cov', 'sky_sum_hgt', 'sky_obs_tot_cov',\\\n","            'sky_low_cld_base_hgt', 'at_pres_altimeter_rate', 'at_pres_stn_rate']]\\\n","                = df.apply(lambda x: updateWeatherCols( x['WND'], x['CIG'], x['VIS'], x['TMP'], x['DEW'], x['SLP'],\\\n","                    x['AA1'], x['GA1'], x['GD1'], x['GF1'], x['MA1']), axis = 1, result_type=\"expand\")\n","\n","    # This list of possible columns are the ones we want to keep\n","    possible_columns = ['airport_code', 'WeatherDtTm', \\\n","        'w_dir_angle', 'w_type', 'w_speed_rate', 'sky_c_hgt', 'sky_c_det', 'sky_c_cavok', \\\n","        'vis_dist', 'vis_var', 'tmp_air', 'tmp_dew', 'sea_lvl_p', \\\n","        'liq_precip_qty', 'liq_precip_dim', 'liq_precip_cond', 'sky_cov', 'sky_cov_base_hgt', 'sky_cov_cld', \\\n","        'sky_sum_cov', 'sky_sum_hgt', 'sky_obs_tot_cov', \\\n","        'sky_low_cld_base_hgt', 'at_pres_altimeter_rate', 'at_pres_stn_rate']\n","\n","    # Compare list of columns to what should be in the final dataframe\n","    weathercols = df.columns.to_list()\n","    dropcols = []\n","    for col in weathercols:\n","        if col not in possible_columns:\n","            dropcols.append(col)\n","\n","    # Drop the columns that aren't in the list of possible_columns\n","    df.drop(dropcols, axis = 1, inplace=True)\n","\n","    # Convert several columns containing numeric values as string to numeric\n","    # Assign the missing value. The values used for missing are those determined and used in the NOAA documentation.\n","    df[['sky_sum_cov', 'liq_precip_cond']] = df[['sky_sum_cov', 'liq_precip_cond']].fillna(9).astype(int)\n","\n","    df[['sky_c_det', 'sky_c_cavok', 'w_type', 'vis_var', 'liq_precip_qty', 'sky_cov', 'sky_cov_cld', 'sky_obs_tot_cov']] =\\\n","        df[['sky_c_det', 'sky_c_cavok', 'w_type', 'vis_var', 'liq_precip_qty', 'sky_cov', 'sky_cov_cld', 'sky_obs_tot_cov']].astype(str)\n","\n","    df[['sky_c_det', 'sky_c_cavok', 'w_type', 'vis_var']] =  df[[ 'sky_c_det', 'sky_c_cavok', 'w_type', 'vis_var']].fillna('9').astype(str)\n","\n","    df[['liq_precip_qty', 'sky_cov', 'sky_cov_cld', 'sky_obs_tot_cov']] = df[['liq_precip_qty', 'sky_cov', 'sky_cov_cld', 'sky_obs_tot_cov']].fillna('99').astype(str)\n","\n","    df[['w_dir_angle']] = df[['w_dir_angle']].fillna(999).astype(int)\n","\n","    df[['w_speed_rate', 'liq_precip_dim', 'tmp_air', 'tmp_dew']] = df[['w_speed_rate', 'liq_precip_dim', 'tmp_air', 'tmp_dew']].fillna(9999).astype(int)\n","\n","    df[['sky_c_hgt', 'sea_lvl_p', 'sky_low_cld_base_hgt', 'at_pres_altimeter_rate', 'at_pres_stn_rate', 'sky_cov_base_hgt', 'sky_sum_hgt' ]] = \\\n","        df[['sky_c_hgt', 'sea_lvl_p', 'sky_low_cld_base_hgt', 'at_pres_altimeter_rate', 'at_pres_stn_rate', 'sky_cov_base_hgt', 'sky_sum_hgt' ]].fillna(99999).astype(int)\n","\n","    df[['vis_dist']] = df[['vis_dist']].fillna(999999).astype(int)\n","\n","    #Items found later, correcting things like 9 vs 9.0 which should be the same\n","    for var in ['sky_c_det', 'w_type', 'sky_c_cavok', 'vis_var']:\n","        df.loc[df[var].isin(['9.0']), var] = '9'\n","\n","    for var in ['liq_precip_qty', 'sky_cov', 'sky_cov_cld', 'sky_obs_tot_cov']:\n","        df.loc[df[var].isin(['99.0']), var] = '99'\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"id":"4b8c94ac","metadata":{"id":"4b8c94ac"},"outputs":[],"source":["#Hourly weather files were moved to subfolder: Weather_year_pkl_FirstTimeThrough, under Hourly:\n","weather_2010 = pd.read_pickle(basepath + \"/Weather/Hourly/Weather_year_pkl_FirstTimeThrough/Weather_2010.pkl\")\n","weather_2010_2 = createYrWeather(weather_2010, '2010')\n","weather_2010_3 = expandColsAndRunFreqs(weather_2010_2)\n","weather_2010_3.to_parquet(basepath + \"/Weather/Hourly/Weather_2010.parquet\")"]},{"cell_type":"code","execution_count":null,"id":"5f2ee84d","metadata":{"id":"5f2ee84d"},"outputs":[],"source":["weather_2011 = pd.read_pickle(basepath + \"/Weather/Hourly/Weather_year_pkl_FirstTimeThrough/Weather_2011.pkl\")\n","weather_2011_2 = createYrWeather(weather_2011, '2011')\n","weather_2011_3 = expandColsAndRunFreqs(weather_2011_2)\n","weather_2011_3.to_parquet(basepath + \"/Weather/Hourly/Weather_2011.parquet\")"]},{"cell_type":"code","execution_count":null,"id":"6b6de8ab","metadata":{"id":"6b6de8ab"},"outputs":[],"source":["weather_2012 = pd.read_pickle(basepath + \"/Weather/Hourly/Weather_year_pkl_FirstTimeThrough/Weather_2012.pkl\")\n","weather_2012_2 = createYrWeather(weather_2012, '2012')\n","weather_2012_3 = expandColsAndRunFreqs(weather_2012_2)\n","weather_2012_3.to_parquet(basepath + \"/Weather/Hourly/Weather_2012.parquet\")"]},{"cell_type":"code","execution_count":null,"id":"be076d06","metadata":{"id":"be076d06"},"outputs":[],"source":["weather_2013 = pd.read_pickle(basepath + \"/Weather/Hourly/Weather_year_pkl_FirstTimeThrough/Weather_2013.pkl\")\n","weather_2013_2 = createYrWeather(weather_2013, '2013')\n","weather_2013_3 = expandColsAndRunFreqs(weather_2013_2)\n","weather_2013_3.to_parquet(basepath + \"/Weather/Hourly/Weather_2013.parquet\")"]},{"cell_type":"code","execution_count":null,"id":"e34b8f9f","metadata":{"id":"e34b8f9f"},"outputs":[],"source":["weather_2014 = pd.read_pickle(basepath + \"/Weather/Hourly/Weather_year_pkl_FirstTimeThrough/Weather_2014.pkl\")\n","weather_2014_2 = createYrWeather(weather_2014, '2014')\n","weather_2014_3 = expandColsAndRunFreqs(weather_2014_2)\n","weather_2014_3.to_parquet(basepath + \"/Weather/Hourly/Weather_2014.parquet\")"]},{"cell_type":"code","execution_count":null,"id":"df26768a","metadata":{"id":"df26768a"},"outputs":[],"source":["weather_2015 = pd.read_pickle(basepath + \"/Weather/Hourly/Weather_year_pkl_FirstTimeThrough/Weather_2015.pkl\")\n","weather_2015_2 = createYrWeather(weather_2015, '2015')\n","weather_2015_3 = expandColsAndRunFreqs(weather_2015_2)\n","weather_2015_3.to_parquet(basepath + \"/Weather/Hourly/Weather_2015.parquet\")"]},{"cell_type":"code","execution_count":null,"id":"684b378e","metadata":{"id":"684b378e"},"outputs":[],"source":["weather_2016 = pd.read_pickle(basepath + \"/Weather/Hourly/Weather_year_pkl_FirstTimeThrough/Weather_2016.pkl\")\n","weather_2016_2 = createYrWeather(weather_2016, '2016')\n","weather_2016_3 = expandColsAndRunFreqs(weather_2016_2)\n","weather_2016_3.to_parquet(basepath + \"/Weather/Hourly/Weather_2016.parquet\")"]},{"cell_type":"code","execution_count":null,"id":"81b0c8df","metadata":{"id":"81b0c8df"},"outputs":[],"source":["weather_2017 = pd.read_pickle(basepath + \"/Weather/Hourly/Weather_year_pkl_FirstTimeThrough/Weather_2017.pkl\")\n","weather_2017_2 = createYrWeather(weather_2017, '2017')\n","weather_2017_3 = expandColsAndRunFreqs(weather_2017_2)\n","weather_2017_3.to_parquet(basepath + \"/Weather/Hourly/Weather_2017.parquet\")"]},{"cell_type":"code","execution_count":null,"id":"2ca7e245","metadata":{"id":"2ca7e245"},"outputs":[],"source":["weather_2018 = pd.read_pickle(basepath + \"/Weather/Hourly/Weather_year_pkl_FirstTimeThrough/Weather_2018.pkl\")\n","weather_2018_2 = createYrWeather(weather_2018, '2018')\n","weather_2018_3 = expandColsAndRunFreqs(weather_2018_2)\n","weather_2018_3.to_parquet(basepath + \"/Weather/Hourly/Weather_2018.parquet\")"]},{"cell_type":"code","execution_count":null,"id":"a9e936b5","metadata":{"id":"a9e936b5"},"outputs":[],"source":["weather_2019 = pd.read_pickle(basepath + \"/Weather/Hourly/Weather_year_pkl_FirstTimeThrough/Weather_2019.pkl\")\n","weather_2019_2 = createYrWeather(weather_2019, '2019')\n","weather_2019_3 = expandColsAndRunFreqs(weather_2019_2)\n","weather_2019_3.to_parquet(basepath + \"/Weather/Hourly/Weather_2019.parquet\")"]},{"cell_type":"code","execution_count":null,"id":"e86af87c","metadata":{"id":"e86af87c"},"outputs":[],"source":["weather_2023 = pd.read_pickle(basepath + \"/Weather/Hourly/Weather_year_pkl_FirstTimeThrough/Weather_2023.pkl\")\n","weather_2023_2 = createYrWeather(weather_2023, '2023')\n","weather_2023_3 = expandColsAndRunFreqs(weather_2023_2)\n","weather_2023_3.to_parquet(basepath + \"/Weather/Hourly/Weather_2023.parquet\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}